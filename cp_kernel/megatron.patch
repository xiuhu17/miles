diff --git a/megatron/core/transformer/dot_product_attention_context_parallel.py b/megatron/core/transformer/dot_product_attention_context_parallel.py
index 89659a1d7..740fd2e2a 100644
--- a/megatron/core/transformer/dot_product_attention_context_parallel.py
+++ b/megatron/core/transformer/dot_product_attention_context_parallel.py
@@ -6,6 +6,7 @@
 
 import torch
 from torch.nn import functional as F
+import torch.distributed as dist
 
 try:
     import einops
@@ -53,12 +54,16 @@ def eager_attn_fwd(q, k, v, attn_bias, sinks, scale, dropout):
 
 
 @torch.no_grad
-def eager_attn_bwd(q, k, v, attn_bias, sinks, scale, dropout, attn_output, probs, grad_output):
+def eager_attn_bwd(q, kv, attn_bias, sinks, scale, dropout, attn_output, probs, grad_output):
     """Backward pass for eager attention"""
 
     # Rearrange query, key, value to (b, h, s, d)
     b, sq, h, d = q.shape
-    sk = k.shape[1]
+    sk = kv.shape[1]
+    k = kv
+    v = kv[:,:,:,:512]
+    q_tail = q[:,:,:,512:]
+    _q_tail_T = einops.rearrange(q_tail, 'b s h d -> b h d s').contiguous()
     _q_T = einops.rearrange(q, 'b s h d -> b h d s')
     _k_T = einops.rearrange(k, 'b s h d -> b h s d')
     _v_T = einops.rearrange(v, ' b s h d -> b h d s')
@@ -70,9 +75,9 @@ def eager_attn_bwd(q, k, v, attn_bias, sinks, scale, dropout, attn_output, probs
         attn_w = probs[..., :-1]  # Drop the sink
     grad_output = einops.rearrange(grad_output, 'b s h d -> b h s d')
     attn_w_T = einops.rearrange(attn_w, ' b h sq sk -> b h sk sq')
-    grad__v = torch.matmul(attn_w_T, grad_output)
-    grad_attn_w = torch.matmul(grad_output, _v_T)
-
+    grad__v = torch.matmul(attn_w_T, grad_output).contiguous() # b h sk d
+    grad_attn_w = torch.matmul(grad_output, _v_T).contiguous() # b h s sk
+ 
     # Backward pass for softmax
     if sinks is None:
         grad_probs = grad_attn_w
@@ -95,15 +100,18 @@ def eager_attn_bwd(q, k, v, attn_bias, sinks, scale, dropout, attn_output, probs
 
     # Backward pass for q @ K^T
     grad_attn_w *= scale
-    grad__q = torch.matmul(grad_attn_w, _k_T)
-    grad__k = torch.matmul(_q_T, grad_attn_w)
+    grad__q = torch.matmul(grad_attn_w, _k_T).contiguous()
+    grad__k = torch.matmul(_q_T, grad_attn_w).contiguous() # b h d sk
+
+    grad__k_T = grad__k.transpose(2, 3).contiguous() # b h sk d
+    grad__kv = torch.zeros((b, h, sk, 576), device=q.device, dtype=q.dtype) # b h sk d
+    grad__kv[:,:,:,:512] = grad__v + grad__k_T[:,:,:,:512]
+    grad__kv[:,:,:,512:] = torch.matmul(_q_tail_T, grad_attn_w).contiguous().transpose(2, 3).contiguous() # b h sk d
 
     # Rearrange grads to (b, s, h, d)
-    grad_v = einops.rearrange(grad__v, 'b h s d -> b s h d')
-    grad_k = einops.rearrange(grad__k, 'b h d s -> b s h d')
+    grad__kv = grad__kv.transpose(1, 2).contiguous()
     grad_q = einops.rearrange(grad__q, 'b h s d -> b s h d')
-    return grad_q, grad_k, grad_v, grad_sinks
-
+    return grad_q, grad__kv, grad_sinks
 
 class AllGatherComm:
     """All gather communication with async operations"""
@@ -132,10 +140,10 @@ class AllGatherComm:
             self.handles = []
 
 
-def to_zz_mask_attn_bias(attention_mask, cp_size, nheads, nheads_k, heads_k_stride, device, dtype):
+def to_zz_mask_attn_bias(attention_mask, cp_size, nheads, nheads_k, heads_k_stride, device, dtype, if_zz_mask=False):
     '''Convert the attention mask to the attention bias'''
 
-    if cp_size == 1:
+    if cp_size == 1 or if_zz_mask:
         zz_mask = attention_mask
     else:
         chunked = attention_mask.chunk(dim=3, chunks=cp_size * 2)
@@ -147,11 +155,11 @@ def to_zz_mask_attn_bias(attention_mask, cp_size, nheads, nheads_k, heads_k_stri
     return attn_bias
 
 
-class AttentionFuncionWithContextParallel(torch.autograd.Function):
+class Ref(torch.autograd.Function):
     """Native attention function with context parallelism."""
 
     @staticmethod
-    def forward(ctx, q, k, v, attention_mask, attention_dropout, softmax_scale, pg):
+    def forward(ctx, q, k, v, attention_mask, attention_dropout, softmax_scale, pg, if_zz_mask=False):
         '''Forward pass for the native attention function with context parallelism'''
 
         # Assert einops exists
@@ -171,12 +179,17 @@ class AttentionFuncionWithContextParallel(torch.autograd.Function):
         probs = []
 
         # Initialize KV buffers
-        kv_buffer = torch.empty(
-            (2, k.shape[0] * cp_size, k.shape[1], heads_k_stride, k.shape[3]),
+        # seperate KV buffer for MLA
+        kv_buffer = [torch.empty(
+            (k.shape[0] * cp_size, k.shape[1], heads_k_stride, k.shape[3]),
             dtype=k.dtype,
             device=k.device,
-        )
-        kv_buffer_copy = torch.empty_like(kv_buffer)
+        ), torch.empty(
+            (v.shape[0] * cp_size, v.shape[1], heads_k_stride, v.shape[3]),
+            dtype=v.dtype,
+            device=v.device,
+        )]
+        kv_buffer_copy = [torch.empty_like(kv_buffer[0]), torch.empty_like(kv_buffer[1])]
 
         # All-gather first chunk of KV buffers
         k_0 = k[:, :, :heads_k_stride].contiguous()
@@ -186,7 +199,7 @@ class AttentionFuncionWithContextParallel(torch.autograd.Function):
 
         # Prepare attention bias
         attn_bias = to_zz_mask_attn_bias(
-            attention_mask, cp_size, nheads, nheads_k, heads_k_stride, q.device, q.dtype
+            attention_mask, cp_size, nheads, nheads_k, heads_k_stride, q.device, q.dtype, if_zz_mask
         )
 
         # Iterate over heads
@@ -215,8 +228,9 @@ class AttentionFuncionWithContextParallel(torch.autograd.Function):
 
             # Forward pass
             out_i, probs_i = eager_attn_fwd(
-                q_i, k_i, v_i, attn_bias, None, softmax_scale, attention_dropout
+                q_i, k_i, v_i, attn_bias.contiguous(), None, softmax_scale, attention_dropout
             )
+
             outs.append(out_i)
             probs.append(probs_i)
 
@@ -226,6 +240,7 @@ class AttentionFuncionWithContextParallel(torch.autograd.Function):
 
         # Save contexts for backward pass
         ctx.save_for_backward(q, k, v, attention_mask, *outs, *probs)
+        ctx.if_zz_mask = if_zz_mask
         ctx.dropout = attention_dropout
         ctx.scale = softmax_scale
         ctx.heads_k_stride = heads_k_stride  # TODO make it configurable
@@ -238,13 +253,13 @@ class AttentionFuncionWithContextParallel(torch.autograd.Function):
         '''Backward pass for the native attention function with context parallelism'''
 
         # Initialize or resume constants and communication group
-        q, k, v, attention_mask, *rest = ctx.saved_tensors
+        q, kv, _, attention_mask, *rest = ctx.saved_tensors
         nheads = q.shape[2]
-        nheads_k = k.shape[2]
-        heads_k_stride = ctx.heads_k_stride
-        assert nheads_k % heads_k_stride == 0
-        outs = rest[: nheads_k // heads_k_stride]
-        probs = rest[nheads_k // heads_k_stride :]
+        nheads_kv = kv.shape[2]
+        heads_kv_stride = ctx.heads_k_stride
+        assert nheads_kv % heads_kv_stride == 0
+        outs = rest[: nheads_kv // heads_kv_stride]
+        probs = rest[nheads_kv // heads_kv_stride :]
         pg = ctx.pg
         cp_size = 1
         if pg is not None:
@@ -253,30 +268,27 @@ class AttentionFuncionWithContextParallel(torch.autograd.Function):
 
         # Initialize KV buffers
         kv_buffer = torch.empty(
-            (2, k.shape[0] * cp_size, k.shape[1], heads_k_stride, k.shape[3]),
-            dtype=k.dtype,
-            device=k.device,
+            (kv.shape[0] * cp_size, kv.shape[1], heads_kv_stride, kv.shape[3]),
+            dtype=kv.dtype,
+            device=kv.device,
         )
         kv_buffer_copy = torch.empty_like(kv_buffer)
 
         # All-gather first chunk of KV buffers
         dq = []
-        dk = []
-        dv = []
-        k_0 = k[:, :, :heads_k_stride].contiguous()
-        v_0 = v[:, :, :heads_k_stride].contiguous()
-        comm.all_gather(kv_buffer_copy[0], k_0)
-        comm.all_gather(kv_buffer_copy[1], v_0)
+        dkv = []
+        kv_0 = kv[:, :, :heads_kv_stride].contiguous()
+        comm.all_gather(kv_buffer_copy, kv_0)
 
         # Prepare attention bias
         attn_bias = to_zz_mask_attn_bias(
-            attention_mask, cp_size, nheads, nheads_k, heads_k_stride, q.device, q.dtype
+            attention_mask, cp_size, nheads, nheads_kv, heads_kv_stride, q.device, q.dtype, ctx.if_zz_mask
         )
 
         # Iterate over heads
-        for i in range(0, nheads_k, heads_k_stride):
+        for i in range(0, nheads_kv, heads_kv_stride):
             # Slice query and output for this iteration
-            q_slice = slice(i * nheads // nheads_k, (i + heads_k_stride) * nheads // nheads_k)
+            q_slice = slice(i * nheads // nheads_kv, (i + heads_kv_stride) * nheads // nheads_kv)
             q_i = q[:, :, q_slice]
             dout_i = dout[:, :, q_slice]
 
@@ -285,58 +297,45 @@ class AttentionFuncionWithContextParallel(torch.autograd.Function):
             kv_buffer, kv_buffer_copy = kv_buffer_copy, kv_buffer
 
             # All-gather the next portion of KV buffers if not the last iteration
-            if i < nheads_k - heads_k_stride:
-                kvsl = i + heads_k_stride
-                kvsr = kvsl + heads_k_stride
-                send_k = k[:, :, kvsl:kvsr].contiguous()
-                send_v = v[:, :, kvsl:kvsr].contiguous()
-                comm.all_gather(kv_buffer_copy[0], send_k)
-                comm.all_gather(kv_buffer_copy[1], send_v)
+            if i < nheads_kv - heads_kv_stride:
+                kvsl = i + heads_kv_stride
+                kvsr = kvsl + heads_kv_stride
+                send_kv = kv[:, :, kvsl:kvsr].contiguous()
+                comm.all_gather(kv_buffer_copy, send_kv)
 
             # Prepare key, value for attention
-            k_i = kv_buffer[0]
-            v_i = kv_buffer[1]
+            kv_i = kv_buffer
 
             # Rearrange query, key, value to (b, s, h, d)
             q_i = einops.rearrange(q_i, 's b h d -> b s h d')
-            k_i = einops.rearrange(k_i, 's b h d -> b s h d')
-            v_i = einops.rearrange(v_i, 's b h d -> b s h d')
+            kv_i = einops.rearrange(kv_i, 's b h d -> b s h d')
             dout_i = einops.rearrange(dout_i, 's b h d -> b s h d')
 
             # Backward pass
-            dq_i, _dk_i, _dv_i, _ = eager_attn_bwd(
-                q_i, k_i, v_i, attn_bias, None, ctx.scale, ctx.dropout, outs[i], probs[i], dout_i
+            dq_i, _dkv_i, _ = eager_attn_bwd(
+                q_i, kv_i, attn_bias, None, ctx.scale, ctx.dropout, outs[i], probs[i], dout_i
             )
 
             # Rearrange gradients to (s, b, h, d)
             dq_i = einops.rearrange(dq_i, 'b s h d -> s b h d')
-            _dk_i = einops.rearrange(_dk_i, 'b s h d -> s b h d')
-            _dv_i = einops.rearrange(_dv_i, 'b s h d -> s b h d')
+            _dkv_i = einops.rearrange(_dkv_i, 'b s h d -> s b h d')
+
             if pg is None:
-                dk_i = _dk_i
-                dv_i = _dv_i
+                dkv_i = _dkv_i
             else:
                 # Reduce-scatter gradients if CP > 1
-                dk_i = torch.zeros(
-                    (k_i.shape[1] // cp_size, k_i.shape[0], k_i.shape[2], k_i.shape[3]),
-                    device=k_i.device,
-                    dtype=k_i.dtype,
-                )
-                dv_i = torch.zeros(
-                    (v_i.shape[1] // cp_size, v_i.shape[0], v_i.shape[2], v_i.shape[3]),
-                    device=v_i.device,
-                    dtype=v_i.dtype,
+                dkv_i = torch.zeros(
+                    (kv_i.shape[1] // cp_size, kv_i.shape[0], kv_i.shape[2], kv_i.shape[3]),
+                    device=kv_i.device,
+                    dtype=kv_i.dtype,
                 )
-                torch.distributed.reduce_scatter_tensor(dk_i, _dk_i, group=pg)
-                torch.distributed.reduce_scatter_tensor(dv_i, _dv_i, group=pg)
+                torch.distributed.reduce_scatter_tensor(dkv_i, _dkv_i, group=pg)
 
             # Collect gradients
             dq.append(dq_i)
-            dk.append(dk_i)
-            dv.append(dv_i)
+            dkv.append(dkv_i)
 
         # Concatenate gradients and return
         dq = torch.cat(dq, dim=2)
-        dk = torch.cat(dk, dim=2)
-        dv = torch.cat(dv, dim=2)
-        return dq, dk, dv, None, None, None, None
+        dkv = torch.cat(dkv, dim=2)
+        return dq, dkv, dkv[:,:,:,:512].contiguous(), None, None, None, None, None
diff --git a/megatron/core/transformer/experimental_attention_variant/dsa.py b/megatron/core/transformer/experimental_attention_variant/dsa.py
index fc994490b..7bc9a485e 100644
--- a/megatron/core/transformer/experimental_attention_variant/dsa.py
+++ b/megatron/core/transformer/experimental_attention_variant/dsa.py
@@ -6,6 +6,7 @@ from dataclasses import dataclass
 from typing import Optional, Tuple, Union
 
 import torch
+import einops
 
 from megatron.core import parallel_state
 from megatron.core.models.common.embeddings import (
@@ -21,6 +22,8 @@ from megatron.core.transformer.module import MegatronModule
 from megatron.core.transformer.spec_utils import ModuleSpec, build_module
 from megatron.core.transformer.transformer_config import TransformerConfig
 
+from megatron.core.transformer.dot_product_attention_context_parallel import AllGatherComm, AttentionFuncionWithContextParallel
+
 try:
     from fast_hadamard_transform import hadamard_transform
 except ImportError:
@@ -191,44 +194,72 @@ def compute_dsa_indexer_loss(
     Returns:
         index_loss: KL divergence loss (scalar).
     """
-    sq, b, np, hn = query.size()
-    sk = key.size(0)
+    cp_size = parallel_state.get_context_parallel_world_size()
 
-    # [sq, b, np, hn] -> [b, np, sq, hn] -> [b * np, sq, hn]
-    query = query.permute(1, 2, 0, 3).reshape(b * np, sq, hn)
-    # [sk, b, np, hn] -> [b, np, hn, sk] -> [b * np, hn, sk]
-    key = key.permute(1, 2, 3, 0).reshape(b * np, hn, sk)
-    # Compute attention scores [b * np, sq, sk]
-    attention_scores = torch.bmm(query.float(), key.float()) * softmax_scale
-    # Reshape to [b, np, sq, sk]
-    attention_scores = attention_scores.reshape(b, np, sq, sk)
+    if cp_size > 1:
+        sq_local, b, np, hn = query.size()
+        sk_local = key.size(0)
+        sk_global = sk_local * cp_size
 
-    # causal_mask [sq, sk]
-    causal_mask = torch.triu(
-        torch.full((sq, sk), float('-inf'), dtype=torch.float32, device=attention_scores.device),
-        diagonal=1,
-    )
-    # index_mask [b, sq, sk]
-    index_mask = torch.full(
-        (b, sq, sk), float("-inf"), dtype=torch.float32, device=causal_mask.device
-    ).scatter_(-1, topk_indices, 0)
-
-    # [b, np, sq, skv] + [1, 1, sq, skv] -> [b, np, sq, skv]
-    attention_scores += causal_mask.view(1, 1, sq, sk)
-    if sparse_loss:
-        # [b, np, sq, sk] + [b, 1, sq, sk] -> [b, np, sq, sk]
-        attention_scores += index_mask.view(b, 1, sq, sk)
-        # [b, sq, sk] + [b, sq, sk] -> [b, sq, sk]
-        index_scores += index_mask
-
-    # [b, np, sq, sk] -> [b, np, sq, sk]
-    attention_scores = torch.nn.functional.softmax(attention_scores, dim=-1, dtype=torch.float32)
-    # [b, sq, sk] -> [b, sq, sk]
-    index_scores = torch.nn.functional.softmax(index_scores, dim=-1, dtype=torch.float32)
+        causal_mask = get_causal_mask(sq_local, sk_local, query.device)
+        float_mask = torch.zeros_like(causal_mask, dtype=torch.float32).masked_fill(
+            causal_mask, float('-inf')
+        )
 
-    # Sum attention scores across heads.
-    # [batch, heads, seqlen_q, seqlen_k] -> [batch, seqlen_q, seqlen_k]
-    attention_scores = attention_scores.sum(dim=1)
+        index_mask = torch.full(
+            (b, sq_local, sk_global), float("-inf"), dtype=torch.float32, device=causal_mask.device
+        ).scatter_(-1, topk_indices, 0)
+
+        float_mask = float_mask.view(1, 1, sq_local, sk_global)
+        float_mask = index_mask.view(b, 1, sq_local, sk_global) + float_mask if sparse_loss else float_mask
+
+        # because the attention computation is more heavy in memory (has head dim),
+        # we apply cp (all-gather backend) on attention scores computation
+        attention_scores = compute_attention_scores_with_cp(query, key, float_mask, softmax_scale) # [b, sq_local, sk_global]
+
+        index_scores = torch.nn.functional.softmax(index_scores, dim=-1, dtype=torch.float32)
+
+    else:
+        sq, b, np, hn = query.size()
+        sk = key.size(0)
+
+        # [sq, b, np, hn] -> [b, np, sq, hn] -> [b * np, sq, hn]
+        query = query.permute(1, 2, 0, 3).reshape(b * np, sq, hn)
+        # [sk, b, np, hn] -> [b, np, hn, sk] -> [b * np, hn, sk]
+        key = key.permute(1, 2, 3, 0).reshape(b * np, hn, sk)
+        # Compute attention scores [b * np, sq, sk]
+        attention_scores = torch.bmm(query.float(), key.float()) * softmax_scale
+        # Reshape to [b, np, sq, sk]
+        attention_scores = attention_scores.reshape(b, np, sq, sk)
+
+        # causal_mask [sq, sk]
+        causal_mask = torch.triu(
+            torch.full((sq, sk), float('-inf'), dtype=torch.float32, device=attention_scores.device),
+            diagonal=1,
+        )
+        # index_mask [b, sq, sk]
+        index_mask = torch.full(
+            (b, sq, sk), float("-inf"), dtype=torch.float32, device=causal_mask.device
+        ).scatter_(-1, topk_indices, 0)
+
+        # [b, np, sq, skv] + [1, 1, sq, skv] -> [b, np, sq, skv]
+        attention_scores += causal_mask.view(1, 1, sq, sk)
+        if sparse_loss:
+            # [b, np, sq, sk] + [b, 1, sq, sk] -> [b, np, sq, sk]
+            attention_scores += index_mask.view(b, 1, sq, sk)
+            # [b, sq, sk] + [b, sq, sk] -> [b, sq, sk]
+            index_scores += index_mask
+
+        # [b, np, sq, sk] -> [b, np, sq, sk]
+        attention_scores = torch.nn.functional.softmax(attention_scores, dim=-1, dtype=torch.float32)
+        # [b, sq, sk] -> [b, sq, sk]
+        index_scores = torch.nn.functional.softmax(index_scores, dim=-1, dtype=torch.float32)
+
+        # Sum attention scores across heads.
+        # [batch, heads, seqlen_q, seqlen_k] -> [batch, seqlen_q, seqlen_k]
+        attention_scores = attention_scores.sum(dim=1)
+
+    # Common part
     if pg_collection.tp.size() > 1:
         # attention scores are scattered to TP ranks in head dimension.
         torch.distributed.all_reduce(attention_scores.contiguous(), group=pg_collection.tp)
@@ -252,6 +283,57 @@ def compute_dsa_indexer_loss(
     return indexer_loss
 
 
+def compute_attention_scores_with_cp(q, k, attn_bias, scale, heads_k_stride = 1):
+    """ 
+    compute attention scores of q_local @ k_global with CP all-gather backend
+    parallel on n_heads dimension 
+    """
+    pg = parallel_state.get_context_parallel_group()
+    cp_size = parallel_state.get_context_parallel_world_size()
+
+    sq_local, b, nheads, hn_q = q.shape
+    sk_local, _, nheads_k, hn_k = k.shape
+    sk_global = sk_local * cp_size
+    
+    assert nheads % nheads_k == 0 and nheads_k % heads_k_stride == 0
+
+    comm = AllGatherComm(group=pg)
+    attns = torch.zeros(b, heads_k_stride, sq_local, sk_global, dtype=q.dtype, device=q.device)
+
+    k_buffer = torch.empty(
+        (sk_global, b, heads_k_stride, hn_k),
+        dtype=k.dtype,
+        device=k.device
+    )
+    k_buffer_copy = torch.empty_like(k_buffer)
+    k_0 = k[:, :, :heads_k_stride].contiguous()
+    comm.all_gather(k_buffer_copy, k_0)
+
+    attn_bias = attn_bias.expand(-1, heads_k_stride * (nheads // nheads_k), -1, -1)
+
+    for i in range(0, nheads_k, heads_k_stride):
+        comm.wait()
+        k_buffer, k_buffer_copy = k_buffer_copy, k_buffer
+        if i < nheads_k - heads_k_stride:
+            kvsl = i + heads_k_stride
+            kvsr = kvsl + heads_k_stride
+            send_k = k[:, :, kvsl:kvsr].contiguous()
+            comm.all_gather(k_buffer_copy, send_k)
+        q_i = q[:, :, i * nheads // nheads_k : (i + heads_k_stride) * nheads // nheads_k]
+        k_i = k_buffer
+
+        _q_i = einops.rearrange(q_i, 's b h d -> b h s d')
+        _k_i = einops.rearrange(k_i, 's b h d -> b h d s')
+        attn_i = torch.matmul(_q_i.float(), _k_i.float()) * scale + attn_bias
+        attn_i = torch.nn.functional.softmax(attn_i, dim=-1, dtype=torch.float32)
+
+        attns = attns + attn_i
+    
+    attns = torch.sum(attns, dim=1)
+
+    return attns
+
+
 class DSAIndexerLossAutoScaler(torch.autograd.Function):
     """An AutoScaler that triggers the backward pass and scales the grad for indexer loss.
 
@@ -496,7 +578,15 @@ class DSAIndexer(MegatronModule):
         # Compute attention scores: q @ k^T
         # [seqlen_q, batch, index_n_heads, index_head_dim] @ [seqlen_k, batch, index_head_dim]^T
         #   -> [seqlen_q, batch, index_n_heads, seqlen_k]
-        index_scores = torch.einsum('sbhd,tbd->sbht', q.float(), k.float())
+        cp_size = parallel_state.get_context_parallel_world_size()
+        if cp_size == 1:
+            index_scores = torch.einsum('sbhd,tbd->sbht', q.float(), k.float())
+        else:
+            # because k is small (only 1 head), do just one all_gather
+            k_buffer = torch.cat(torch.distributed.nn.functional.all_gather(k, group=self.pg_collection.cp), dim=0)  # k_buffer: [[chunk_0, chunk_3, chunk_1, chunk_2], batch, index_head_dim]
+            index_scores = torch.einsum('sbhd,tbd->sbht', q.float(), k_buffer.float()) # [s_q_local, batch, index_n_heads, s_k_global]
+            # rank 0: q [chunk_0, chunk_3], k[chunk_0, chunk_3, chunk_1, chunk_2]
+            # rank 1: q [chunk_1, chunk_2], k[chunk_0, chunk_3, chunk_1, chunk_2]
 
         # Apply ReLU activation.
         index_scores = torch.relu(index_scores)
@@ -606,7 +696,10 @@ class DSAIndexer(MegatronModule):
         # =========================================
         # Select top-k indices
         # =========================================
-        topk_k = min(self.index_topk, seqlen)
+        cp_size = parallel_state.get_context_parallel_world_size()
+
+        seqlen_k_global = k.shape[0] * cp_size
+        topk_k = min(self.index_topk, seqlen_k_global)
         # [batch, seqlen, index_topk]
         topk_indices = index_scores.topk(topk_k, dim=-1)[1]
 
@@ -687,6 +780,57 @@ def unfused_dsa_fn(query, key, value, topk_indices, softmax_scale):
     output = output.reshape(sq, b, np * hnv)
     return output
 
+def get_causal_mask(sq, skv, device):
+    cp_size = parallel_state.get_context_parallel_world_size()
+    cp_rank = parallel_state.get_context_parallel_rank()
+    skv_global = skv * cp_size
+
+    if cp_size == 1:
+        causal_mask = torch.triu(
+            torch.ones((sq, skv), dtype=torch.bool, device=device),
+            diagonal=1,
+        )
+    else:
+        sq_half = sq // 2
+        global_q_positions = torch.cat([
+            torch.arange(cp_rank * sq_half, (cp_rank + 1) * sq_half, device=device),
+            torch.arange(skv_global - (cp_rank + 1) * sq_half, skv_global - cp_rank * sq_half, device=device)
+        ])
+        
+        global_k_positions = torch.arange(skv_global, device=device)
+        # [sq, 1] < [1, skv_global] -> [sq, skv_global]
+        causal_mask = global_q_positions.unsqueeze(1) < global_k_positions.unsqueeze(0)
+        # convert to zz mask
+        chunked = causal_mask.chunk(dim=1, chunks=cp_size * 2)
+        causal_mask = [_x for _p in zip(chunked[:cp_size], reversed(chunked[cp_size:])) for _x in _p]
+        causal_mask = torch.cat(causal_mask, dim=1)
+
+    return causal_mask
+
+def unfused_dsa_fn_with_cp(query, key, value, topk_indices, softmax_scale):
+    pg = parallel_state.get_context_parallel_group()
+    cp_size = parallel_state.get_context_parallel_world_size()
+    cp_rank = parallel_state.get_context_parallel_rank()
+
+    sq, b, np, hn = query.size()
+    skv = key.size(0)
+    hnv = value.size(3)
+    
+    skv_global = skv * cp_size
+    
+    sparse_mask = torch.ones((b, sq, skv_global), dtype=torch.bool, device=query.device)
+    sparse_mask.scatter_(-1, topk_indices, False)
+    
+    causal_mask = get_causal_mask(sq, skv, query.device)
+   
+    combined_mask = sparse_mask | causal_mask.unsqueeze(0)
+
+    attention_mask_for_cp = combined_mask.unsqueeze(1)  # [b, 1, sq, skv_global]
+    output = AttentionFuncionWithContextParallel.apply(
+        query, key, value, attention_mask_for_cp, 0.0, softmax_scale, pg, True
+    )
+    return output.reshape(sq, b, np * hnv)
+
 
 class DSAttention(MegatronModule):
     """
@@ -768,18 +912,17 @@ class DSAttention(MegatronModule):
             # Generate upper triangular mask with -inf above diagonal, 0 elsewhere
             # torch.triu with diagonal=1 creates upper triangular matrix (excluding main diagonal)
             # float_mask [sq, skv]
-            float_mask = torch.triu(
-                torch.full((sq, skv), float('-inf'), dtype=torch.float32, device=x.device),
-                diagonal=1,
-            )
+            mask = get_causal_mask(sq, skv, x.device)
         else:
-            assert attention_mask.shape == (b, 1, sq, skv), 'attention_mask shape mismatch'
+            skv_global = skv * parallel_state.get_context_parallel_world_size()
+            assert attention_mask.shape == (b, 1, sq, skv_global), 'attention_mask shape mismatch'
             # [b, 1, sq, skv] -> [b, sq, skv]
             mask = attention_mask.squeeze()
-            # float_mask [b, sq, skv]
-            float_mask = torch.zeros_like(mask, dtype=torch.float32).masked_fill(
-                mask, float('-inf')
-            )
+
+        # float_mask [b, sq, skv]
+        float_mask = torch.zeros_like(mask, dtype=torch.float32).masked_fill(
+            mask, float('-inf')
+        )
 
         # ===================================
         # Get index scores and top-k indices
@@ -791,7 +934,7 @@ class DSAttention(MegatronModule):
         # ===================================
         # Run sparse attention kernel
         # ===================================
-        output = unfused_dsa_fn(query, key, value, topk_indices, self.softmax_scale)
+        output = unfused_dsa_fn_with_cp(query, key, value, topk_indices, self.softmax_scale)
 
         # ===================================
         # Attach indexer loss
diff --git a/megatron/core/transformer/multi_latent_attention.py b/megatron/core/transformer/multi_latent_attention.py
index 3953d933b..84301ed54 100644
--- a/megatron/core/transformer/multi_latent_attention.py
+++ b/megatron/core/transformer/multi_latent_attention.py
@@ -6,6 +6,7 @@ from dataclasses import dataclass
 from typing import NoReturn, Optional, Union
 
 import torch
+import torch.nn.functional as F
 
 try:
     from einops import rearrange
diff --git a/megatron/core/transformer/transformer_config.py b/megatron/core/transformer/transformer_config.py
index a3a167549..98391fda6 100644
--- a/megatron/core/transformer/transformer_config.py
+++ b/megatron/core/transformer/transformer_config.py
@@ -918,9 +918,9 @@ class TransformerConfig(ModelParallelConfig):
                     f" but got {self.context_parallel_size=}."
                 )
         elif self.experimental_attention_variant == "dsa":
-            assert (
-                self.context_parallel_size == 1
-            ), "Currently context parallelism is not supported by DSAttention!"
+            # assert (
+            #     self.context_parallel_size == 1
+            # ), "Currently context parallelism is not supported by DSAttention!"
             assert not self.apply_rope_fusion, "RoPE fusion is not supported for DSAttention"
 
         if self.fp8:
