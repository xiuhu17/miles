diff --git a/megatron/core/transformer/dot_product_attention_context_parallel.py b/megatron/core/transformer/dot_product_attention_context_parallel.py
index 89659a1d7..b9bd38718 100644
--- a/megatron/core/transformer/dot_product_attention_context_parallel.py
+++ b/megatron/core/transformer/dot_product_attention_context_parallel.py
@@ -3,9 +3,12 @@
 # Some of this code was adopted from https://github.com/zhuzilin/ring-flash-attention/
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
+# Kernel is adpoted from tilelang/examples/deepseek_v32
 
 import torch
+import torch.distributed as dist
 from torch.nn import functional as F
+from .tilelang_kernel import sparse_mla_fwd_interface, sparse_mla_bwd
 
 try:
     import einops
@@ -15,96 +18,6 @@ except ImportError:
     HAVE_EINOPS = False
 
 
-@torch.no_grad
-def eager_attn_fwd(q, k, v, attn_bias, sinks, scale, dropout):
-    """Forward pass for eager attention"""
-
-    # Rearrange query, key, value to (b, h, s, d)
-    b, sq, h, d = q.shape
-    sk = k.shape[1]
-    _q = einops.rearrange(q, 'b s h d -> b h s d')
-    _k = einops.rearrange(k, 'b s h d -> b h d s')
-    _v = einops.rearrange(v, 'b s h d -> b h s d')
-
-    # Compute attention weights
-    attn_w = torch.matmul(_q, _k) * scale
-    attn_w = attn_w + attn_bias
-
-    # Add sinks to attention weights
-    if sinks is None:
-        logits = attn_w
-    else:
-        _sinks = sinks.reshape(1, h, 1, 1).expand(b, -1, sq, 1)
-        logits = torch.cat([attn_w, _sinks], dim=-1)
-
-    # Compute attention scores
-    probs = F.softmax(logits, dim=-1, dtype=logits.dtype)
-    if sinks is None:
-        attn_w = probs
-    else:
-        attn_w = probs[..., :-1]  # Drop the sink
-
-    # Compute attention output
-    attn_output = torch.matmul(attn_w, _v)
-    attn_output = einops.rearrange(attn_output, 'b h s d -> b s h d')
-    attn_output = attn_output.contiguous()
-
-    return attn_output, probs
-
-
-@torch.no_grad
-def eager_attn_bwd(q, k, v, attn_bias, sinks, scale, dropout, attn_output, probs, grad_output):
-    """Backward pass for eager attention"""
-
-    # Rearrange query, key, value to (b, h, s, d)
-    b, sq, h, d = q.shape
-    sk = k.shape[1]
-    _q_T = einops.rearrange(q, 'b s h d -> b h d s')
-    _k_T = einops.rearrange(k, 'b s h d -> b h s d')
-    _v_T = einops.rearrange(v, ' b s h d -> b h d s')
-
-    # Backward pass for score @ value
-    if sinks is None:
-        attn_w = probs
-    else:
-        attn_w = probs[..., :-1]  # Drop the sink
-    grad_output = einops.rearrange(grad_output, 'b s h d -> b h s d')
-    attn_w_T = einops.rearrange(attn_w, ' b h sq sk -> b h sk sq')
-    grad__v = torch.matmul(attn_w_T, grad_output)
-    grad_attn_w = torch.matmul(grad_output, _v_T)
-
-    # Backward pass for softmax
-    if sinks is None:
-        grad_probs = grad_attn_w
-    else:
-        dummy = torch.zeros((b, h, sq, 1), device=q.device, dtype=q.dtype)
-        grad_probs = torch.cat([grad_attn_w, dummy], dim=3)
-    del grad_attn_w
-    grad_logits = torch._softmax_backward_data(
-        grad_probs, probs, -1, probs.dtype
-    )  # [b, h, sq, sk+1]
-
-    # Backward pass for adding sinks
-    if sinks is None:
-        grad_sinks = None
-        grad_attn_w = grad_logits
-    else:
-        grad__sinks = grad_logits[:, :, :, -1]  # [b, h, sq]
-        grad_sinks = einops.rearrange(grad__sinks, 'b h s -> h (b s)').sum(-1)
-        grad_attn_w = grad_logits[:, :, :, :-1].contiguous()  # [b, h, sq, sk]
-
-    # Backward pass for q @ K^T
-    grad_attn_w *= scale
-    grad__q = torch.matmul(grad_attn_w, _k_T)
-    grad__k = torch.matmul(_q_T, grad_attn_w)
-
-    # Rearrange grads to (b, s, h, d)
-    grad_v = einops.rearrange(grad__v, 'b h s d -> b s h d')
-    grad_k = einops.rearrange(grad__k, 'b h d s -> b s h d')
-    grad_q = einops.rearrange(grad__q, 'b h s d -> b s h d')
-    return grad_q, grad_k, grad_v, grad_sinks
-
-
 class AllGatherComm:
     """All gather communication with async operations"""
 
@@ -131,27 +44,17 @@ class AllGatherComm:
                 handle.wait()
             self.handles = []
 
-
-def to_zz_mask_attn_bias(attention_mask, cp_size, nheads, nheads_k, heads_k_stride, device, dtype):
-    '''Convert the attention mask to the attention bias'''
-
-    if cp_size == 1:
-        zz_mask = attention_mask
-    else:
-        chunked = attention_mask.chunk(dim=3, chunks=cp_size * 2)
-        zz_mask = [_x for _p in zip(chunked[:cp_size], reversed(chunked[cp_size:])) for _x in _p]
-        zz_mask = torch.cat(zz_mask, dim=3)
-    attn_bias = torch.zeros(zz_mask.shape, device=device, dtype=dtype)
-    attn_bias.masked_fill_(zz_mask, float('-inf'))
-    attn_bias = attn_bias.expand(-1, heads_k_stride * (nheads // nheads_k), -1, -1)
-    return attn_bias
-
-
 class AttentionFuncionWithContextParallel(torch.autograd.Function):
     """Native attention function with context parallelism."""
 
+    # q: [seq_len_shard, batch, nheads, dim + tail_dim]
+    # kv: [seq_len_kv_shard, batch, kv_group, dim + tail_dim]
+    #   k: [seq_len_kv_shard, batch, kv_group, dim + tail_dim]
+    #   v: [seq_len_kv_shard, batch, kv_group, dim]
+    # indices: [batch, kv_group, seq_len, topk]
+    # masks: [batch, kv_group, seq_len, seq_len_kv]
     @staticmethod
-    def forward(ctx, q, k, v, attention_mask, attention_dropout, softmax_scale, pg):
+    def forward(ctx, q, kv, v, indices, masks, dim_v, K, attention_dropout, softmax_scale, pg):
         '''Forward pass for the native attention function with context parallelism'''
 
         # Assert einops exists
@@ -164,72 +67,75 @@ class AttentionFuncionWithContextParallel(torch.autograd.Function):
             cp_size = torch.distributed.get_world_size(pg)
         comm = AllGatherComm(group=pg)
         nheads = q.shape[2]
-        nheads_k = k.shape[2]
-        heads_k_stride = 1
-        assert nheads % nheads_k == 0 and nheads_k % heads_k_stride == 0
+        kv_group = kv.shape[2]
+        heads_kv_stride = 1
+        assert nheads % kv_group == 0 and kv_group % heads_kv_stride == 0
         outs = []
-        probs = []
+        lses = []
 
         # Initialize KV buffers
         kv_buffer = torch.empty(
-            (2, k.shape[0] * cp_size, k.shape[1], heads_k_stride, k.shape[3]),
-            dtype=k.dtype,
-            device=k.device,
+            (kv.shape[0] * cp_size, kv.shape[1], heads_kv_stride, kv.shape[3]),
+            dtype=kv.dtype,
+            device=kv.device,
         )
         kv_buffer_copy = torch.empty_like(kv_buffer)
 
         # All-gather first chunk of KV buffers
-        k_0 = k[:, :, :heads_k_stride].contiguous()
-        v_0 = v[:, :, :heads_k_stride].contiguous()
-        comm.all_gather(kv_buffer_copy[0], k_0)
-        comm.all_gather(kv_buffer_copy[1], v_0)
-
-        # Prepare attention bias
-        attn_bias = to_zz_mask_attn_bias(
-            attention_mask, cp_size, nheads, nheads_k, heads_k_stride, q.device, q.dtype
-        )
-
-        # Iterate over heads
-        for i in range(0, nheads_k, heads_k_stride):
+        kv_0 = kv[:, :, :heads_kv_stride].contiguous()
+        comm.all_gather(kv_buffer_copy, kv_0)
+
+        # Prepare topk
+        zz_indices = indices.transpose(1, 2)
+        zz_masks = masks.transpose(1, 2)
+        
+        # Iterate over heads, sequential, i
+        for i in range(0, kv_group, heads_kv_stride):
             # Wait for previous all-gather to complete
             comm.wait()
             kv_buffer, kv_buffer_copy = kv_buffer_copy, kv_buffer
             # All-gather the next portion of KV buffers if not the last iteration
-            if i < nheads_k - heads_k_stride:
-                kvsl = i + heads_k_stride
-                kvsr = kvsl + heads_k_stride
-                send_k = k[:, :, kvsl:kvsr].contiguous()
-                send_v = v[:, :, kvsl:kvsr].contiguous()
-                comm.all_gather(kv_buffer_copy[0], send_k)
-                comm.all_gather(kv_buffer_copy[1], send_v)
+            if i < kv_group - heads_kv_stride:
+                kvsl = i + heads_kv_stride
+                kvsr = kvsl + heads_kv_stride
+                send_kv = kv[:, :, kvsl:kvsr].contiguous()
+                comm.all_gather(kv_buffer_copy, send_kv)
 
             # Prepare query, key, value for attention
-            q_i = q[:, :, i * nheads // nheads_k : (i + heads_k_stride) * nheads // nheads_k]
-            k_i = kv_buffer[0]
-            v_i = kv_buffer[1]
+            q_i = q[:, :, i * nheads // kv_group : (i + heads_kv_stride) * nheads // kv_group]
+            kv_i = kv_buffer
 
             # Rearrange query, key, value to (b, s, h, d)
             q_i = einops.rearrange(q_i, 's b h d -> b s h d')
-            k_i = einops.rearrange(k_i, 's b h d -> b s h d')
-            v_i = einops.rearrange(v_i, 's b h d -> b s h d')
+            s_, b_, h_, d_ = kv_i.shape
+            kv_i = einops.rearrange(kv_i, 's b h d -> b s h d').flatten().view(b_, s_, h_, d_)
+            zz_indices_i = zz_indices[:, :, i:(i+heads_kv_stride)]
+            b_, s_, g_, topk_ = zz_indices_i.shape
+            zz_indices_i = zz_indices_i.flatten().view(b_, s_, g_, topk_)
+            zz_masks_i =  zz_masks[:, :, i:(i+heads_kv_stride)]
+            b_, s_, g_, skv_ = zz_masks_i.shape
+            zz_masks_i = zz_masks_i.flatten().view(b_, s_, g_, skv_)
 
             # Forward pass
-            out_i, probs_i = eager_attn_fwd(
-                q_i, k_i, v_i, attn_bias, None, softmax_scale, attention_dropout
-            )
-            outs.append(out_i)
-            probs.append(probs_i)
+            out_i, lse_i = sparse_mla_fwd_interface(q_i.contiguous(), kv_i, zz_indices_i, zz_masks_i, dim_v, sm_scale = softmax_scale)
+
+            outs.append(out_i.contiguous())
+            lses.append(lse_i.contiguous())
 
-        # Concatenate outputs and rearrange to (s, b, h, d)
+        # out: [B, seq_len_shard, h, dim] -> [seq_len, B, h, dim]
         out = torch.cat(outs, dim=2)
         out = einops.rearrange(out, 'b s h d -> s b h d')
 
         # Save contexts for backward pass
-        ctx.save_for_backward(q, k, v, attention_mask, *outs, *probs)
+        # outs: [[B, seq_len_shard, nheads // kv_group, dim], ...., [B, seq_len_shard, nheads // kv_group, dim]], repeat kv_group // heads_kv_stride times
+        # lses: [[B, seq_len_shard, heads_kv_stride], ...., [B, seq_len_shard, heads_kv_stride]], repeat kv_group // heads_kv_stride times
+        ctx.save_for_backward(q, kv, indices, masks, *outs, *lses)
+        ctx.K = K
         ctx.dropout = attention_dropout
-        ctx.scale = softmax_scale
-        ctx.heads_k_stride = heads_k_stride  # TODO make it configurable
+        ctx.softmax_scale = softmax_scale
+        ctx.heads_kv_stride = heads_kv_stride  # TODO make it configurable
         ctx.pg = pg
+        ctx.dim_v = dim_v
 
         return out
 
@@ -238,13 +144,18 @@ class AttentionFuncionWithContextParallel(torch.autograd.Function):
         '''Backward pass for the native attention function with context parallelism'''
 
         # Initialize or resume constants and communication group
-        q, k, v, attention_mask, *rest = ctx.saved_tensors
+        q, kv, indices, masks, *rest = ctx.saved_tensors
+        K = ctx.K
+        dim_v = ctx.dim_v
         nheads = q.shape[2]
-        nheads_k = k.shape[2]
-        heads_k_stride = ctx.heads_k_stride
-        assert nheads_k % heads_k_stride == 0
-        outs = rest[: nheads_k // heads_k_stride]
-        probs = rest[nheads_k // heads_k_stride :]
+        kv_group = kv.shape[2]
+        heads_kv_stride = ctx.heads_kv_stride
+        softmax_scale = ctx.softmax_scale
+        assert kv_group % heads_kv_stride == 0
+
+        outs = rest[: kv_group // heads_kv_stride]
+        lses = rest[kv_group // heads_kv_stride :]
+
         pg = ctx.pg
         cp_size = 1
         if pg is not None:
@@ -253,30 +164,26 @@ class AttentionFuncionWithContextParallel(torch.autograd.Function):
 
         # Initialize KV buffers
         kv_buffer = torch.empty(
-            (2, k.shape[0] * cp_size, k.shape[1], heads_k_stride, k.shape[3]),
-            dtype=k.dtype,
-            device=k.device,
+            (kv.shape[0] * cp_size, kv.shape[1], heads_kv_stride, kv.shape[3]),
+            dtype=kv.dtype,
+            device=kv.device,
         )
         kv_buffer_copy = torch.empty_like(kv_buffer)
 
         # All-gather first chunk of KV buffers
         dq = []
-        dk = []
-        dv = []
-        k_0 = k[:, :, :heads_k_stride].contiguous()
-        v_0 = v[:, :, :heads_k_stride].contiguous()
-        comm.all_gather(kv_buffer_copy[0], k_0)
-        comm.all_gather(kv_buffer_copy[1], v_0)
-
-        # Prepare attention bias
-        attn_bias = to_zz_mask_attn_bias(
-            attention_mask, cp_size, nheads, nheads_k, heads_k_stride, q.device, q.dtype
-        )
+        dkv = []
+        kv_0 = kv[:, :, :heads_kv_stride].contiguous()
+        comm.all_gather(kv_buffer_copy, kv_0)
+
+        # Prepare topk
+        zz_indices = indices.transpose(1, 2)
+        zz_masks = masks.transpose(1, 2)
 
         # Iterate over heads
-        for i in range(0, nheads_k, heads_k_stride):
+        for i in range(0, kv_group, heads_kv_stride):
             # Slice query and output for this iteration
-            q_slice = slice(i * nheads // nheads_k, (i + heads_k_stride) * nheads // nheads_k)
+            q_slice = slice(i * nheads // kv_group, (i + heads_kv_stride) * nheads // kv_group)
             q_i = q[:, :, q_slice]
             dout_i = dout[:, :, q_slice]
 
@@ -285,58 +192,50 @@ class AttentionFuncionWithContextParallel(torch.autograd.Function):
             kv_buffer, kv_buffer_copy = kv_buffer_copy, kv_buffer
 
             # All-gather the next portion of KV buffers if not the last iteration
-            if i < nheads_k - heads_k_stride:
-                kvsl = i + heads_k_stride
-                kvsr = kvsl + heads_k_stride
-                send_k = k[:, :, kvsl:kvsr].contiguous()
-                send_v = v[:, :, kvsl:kvsr].contiguous()
-                comm.all_gather(kv_buffer_copy[0], send_k)
-                comm.all_gather(kv_buffer_copy[1], send_v)
+            if i < kv_group - heads_kv_stride:
+                kvsl = i + heads_kv_stride
+                kvsr = kvsl + heads_kv_stride
+                send_kv = kv[:, :, kvsl:kvsr].contiguous()
+                comm.all_gather(kv_buffer_copy, send_kv)
 
             # Prepare key, value for attention
-            k_i = kv_buffer[0]
-            v_i = kv_buffer[1]
+            kv_i = kv_buffer
 
             # Rearrange query, key, value to (b, s, h, d)
             q_i = einops.rearrange(q_i, 's b h d -> b s h d')
-            k_i = einops.rearrange(k_i, 's b h d -> b s h d')
-            v_i = einops.rearrange(v_i, 's b h d -> b s h d')
+            s_, b_, h_, d_ = kv_i.shape
+            kv_i = einops.rearrange(kv_i, 's b h d -> b s h d').flatten().view(b_, s_, h_, d_)
             dout_i = einops.rearrange(dout_i, 's b h d -> b s h d')
+            zz_indices_i = zz_indices[:, :, i:(i+heads_kv_stride)]
+            b_, s_, g_, topk_ = zz_indices_i.shape
+            zz_indices_i = zz_indices_i.flatten().view(b_, s_, g_, topk_)
+            zz_masks_i =  zz_masks[:, :, i:(i+heads_kv_stride)]
+            b_, s_, g_, skv_ = zz_masks_i.shape
+            zz_masks_i = zz_masks_i.flatten().view(b_, s_, g_, skv_)
 
             # Backward pass
-            dq_i, _dk_i, _dv_i, _ = eager_attn_bwd(
-                q_i, k_i, v_i, attn_bias, None, ctx.scale, ctx.dropout, outs[i], probs[i], dout_i
-            )
-
+            # TODO: needs casual = True, may not be compatible with zz
+            dq_i, _dkv_i = sparse_mla_bwd(q_i.contiguous(), kv_i, outs[i], dout_i.contiguous(), zz_indices_i, zz_masks_i, lses[i], dim_v, softmax_scale, True)
+            
             # Rearrange gradients to (s, b, h, d)
             dq_i = einops.rearrange(dq_i, 'b s h d -> s b h d')
-            _dk_i = einops.rearrange(_dk_i, 'b s h d -> s b h d')
-            _dv_i = einops.rearrange(_dv_i, 'b s h d -> s b h d')
+            _dkv_i = einops.rearrange(_dkv_i, 'b s h d -> s b h d')
             if pg is None:
-                dk_i = _dk_i
-                dv_i = _dv_i
+                dkv_i = _dkv_i
             else:
                 # Reduce-scatter gradients if CP > 1
-                dk_i = torch.zeros(
-                    (k_i.shape[1] // cp_size, k_i.shape[0], k_i.shape[2], k_i.shape[3]),
-                    device=k_i.device,
-                    dtype=k_i.dtype,
-                )
-                dv_i = torch.zeros(
-                    (v_i.shape[1] // cp_size, v_i.shape[0], v_i.shape[2], v_i.shape[3]),
-                    device=v_i.device,
-                    dtype=v_i.dtype,
+                dkv_i = torch.zeros(
+                    (kv_i.shape[1] // cp_size, kv_i.shape[0], kv_i.shape[2], kv_i.shape[3]),
+                    device=kv_i.device,
+                    dtype=kv_i.dtype,
                 )
-                torch.distributed.reduce_scatter_tensor(dk_i, _dk_i, group=pg)
-                torch.distributed.reduce_scatter_tensor(dv_i, _dv_i, group=pg)
+                torch.distributed.reduce_scatter_tensor(dkv_i, _dkv_i, group=pg)
 
             # Collect gradients
             dq.append(dq_i)
-            dk.append(dk_i)
-            dv.append(dv_i)
+            dkv.append(dkv_i)
 
         # Concatenate gradients and return
         dq = torch.cat(dq, dim=2)
-        dk = torch.cat(dk, dim=2)
-        dv = torch.cat(dv, dim=2)
-        return dq, dk, dv, None, None, None, None
+        dkv = torch.cat(dkv, dim=2)
+        return dq, dkv, dkv[:,:,:,:dim_v].detach().contiguous(), None, None, None, None, None, None, None
diff --git a/megatron/core/transformer/experimental_attention_variant/dsa.py b/megatron/core/transformer/experimental_attention_variant/dsa.py
index fc994490b..c64fa7d4e 100644
--- a/megatron/core/transformer/experimental_attention_variant/dsa.py
+++ b/megatron/core/transformer/experimental_attention_variant/dsa.py
@@ -6,6 +6,7 @@ from dataclasses import dataclass
 from typing import Optional, Tuple, Union
 
 import torch
+import einops
 
 from megatron.core import parallel_state
 from megatron.core.models.common.embeddings import (
@@ -21,6 +22,8 @@ from megatron.core.transformer.module import MegatronModule
 from megatron.core.transformer.spec_utils import ModuleSpec, build_module
 from megatron.core.transformer.transformer_config import TransformerConfig
 
+from megatron.core.transformer.dot_product_attention_context_parallel import AllGatherComm, AttentionFuncionWithContextParallel
+
 try:
     from fast_hadamard_transform import hadamard_transform
 except ImportError:
@@ -191,44 +194,72 @@ def compute_dsa_indexer_loss(
     Returns:
         index_loss: KL divergence loss (scalar).
     """
-    sq, b, np, hn = query.size()
-    sk = key.size(0)
+    cp_size = parallel_state.get_context_parallel_world_size()
 
-    # [sq, b, np, hn] -> [b, np, sq, hn] -> [b * np, sq, hn]
-    query = query.permute(1, 2, 0, 3).reshape(b * np, sq, hn)
-    # [sk, b, np, hn] -> [b, np, hn, sk] -> [b * np, hn, sk]
-    key = key.permute(1, 2, 3, 0).reshape(b * np, hn, sk)
-    # Compute attention scores [b * np, sq, sk]
-    attention_scores = torch.bmm(query.float(), key.float()) * softmax_scale
-    # Reshape to [b, np, sq, sk]
-    attention_scores = attention_scores.reshape(b, np, sq, sk)
+    if cp_size > 1:
+        sq_local, b, np, hn = query.size()
+        sk_local = key.size(0)
+        sk_global = sk_local * cp_size
 
-    # causal_mask [sq, sk]
-    causal_mask = torch.triu(
-        torch.full((sq, sk), float('-inf'), dtype=torch.float32, device=attention_scores.device),
-        diagonal=1,
-    )
-    # index_mask [b, sq, sk]
-    index_mask = torch.full(
-        (b, sq, sk), float("-inf"), dtype=torch.float32, device=causal_mask.device
-    ).scatter_(-1, topk_indices, 0)
-
-    # [b, np, sq, skv] + [1, 1, sq, skv] -> [b, np, sq, skv]
-    attention_scores += causal_mask.view(1, 1, sq, sk)
-    if sparse_loss:
-        # [b, np, sq, sk] + [b, 1, sq, sk] -> [b, np, sq, sk]
-        attention_scores += index_mask.view(b, 1, sq, sk)
-        # [b, sq, sk] + [b, sq, sk] -> [b, sq, sk]
-        index_scores += index_mask
-
-    # [b, np, sq, sk] -> [b, np, sq, sk]
-    attention_scores = torch.nn.functional.softmax(attention_scores, dim=-1, dtype=torch.float32)
-    # [b, sq, sk] -> [b, sq, sk]
-    index_scores = torch.nn.functional.softmax(index_scores, dim=-1, dtype=torch.float32)
+        causal_mask = get_causal_mask(sq_local, sk_local, query.device)
+        float_mask = torch.zeros_like(causal_mask, dtype=torch.float32).masked_fill(
+            causal_mask, float('-inf')
+        )
 
-    # Sum attention scores across heads.
-    # [batch, heads, seqlen_q, seqlen_k] -> [batch, seqlen_q, seqlen_k]
-    attention_scores = attention_scores.sum(dim=1)
+        index_mask = torch.full(
+            (b, sq_local, sk_global), float("-inf"), dtype=torch.float32, device=causal_mask.device
+        ).scatter_(-1, topk_indices, 0)
+
+        float_mask = float_mask.view(1, 1, sq_local, sk_global)
+        float_mask = index_mask.view(b, 1, sq_local, sk_global) + float_mask if sparse_loss else float_mask
+
+        # because the attention computation is more heavy in memory (has head dim),
+        # we apply cp (all-gather backend) on attention scores computation
+        attention_scores = compute_attention_scores_with_cp(query, key, float_mask, softmax_scale) # [b, sq_local, sk_global]
+
+        index_scores = torch.nn.functional.softmax(index_scores, dim=-1, dtype=torch.float32)
+
+    else:
+        sq, b, np, hn = query.size()
+        sk = key.size(0)
+
+        # [sq, b, np, hn] -> [b, np, sq, hn] -> [b * np, sq, hn]
+        query = query.permute(1, 2, 0, 3).reshape(b * np, sq, hn)
+        # [sk, b, np, hn] -> [b, np, hn, sk] -> [b * np, hn, sk]
+        key = key.permute(1, 2, 3, 0).reshape(b * np, hn, sk)
+        # Compute attention scores [b * np, sq, sk]
+        attention_scores = torch.bmm(query.float(), key.float()) * softmax_scale
+        # Reshape to [b, np, sq, sk]
+        attention_scores = attention_scores.reshape(b, np, sq, sk)
+
+        # causal_mask [sq, sk]
+        causal_mask = torch.triu(
+            torch.full((sq, sk), float('-inf'), dtype=torch.float32, device=attention_scores.device),
+            diagonal=1,
+        )
+        # index_mask [b, sq, sk]
+        index_mask = torch.full(
+            (b, sq, sk), float("-inf"), dtype=torch.float32, device=causal_mask.device
+        ).scatter_(-1, topk_indices, 0)
+
+        # [b, np, sq, skv] + [1, 1, sq, skv] -> [b, np, sq, skv]
+        attention_scores += causal_mask.view(1, 1, sq, sk)
+        if sparse_loss:
+            # [b, np, sq, sk] + [b, 1, sq, sk] -> [b, np, sq, sk]
+            attention_scores += index_mask.view(b, 1, sq, sk)
+            # [b, sq, sk] + [b, sq, sk] -> [b, sq, sk]
+            index_scores += index_mask
+
+        # [b, np, sq, sk] -> [b, np, sq, sk]
+        attention_scores = torch.nn.functional.softmax(attention_scores, dim=-1, dtype=torch.float32)
+        # [b, sq, sk] -> [b, sq, sk]
+        index_scores = torch.nn.functional.softmax(index_scores, dim=-1, dtype=torch.float32)
+
+        # Sum attention scores across heads.
+        # [batch, heads, seqlen_q, seqlen_k] -> [batch, seqlen_q, seqlen_k]
+        attention_scores = attention_scores.sum(dim=1)
+
+    # Common part
     if pg_collection.tp.size() > 1:
         # attention scores are scattered to TP ranks in head dimension.
         torch.distributed.all_reduce(attention_scores.contiguous(), group=pg_collection.tp)
@@ -252,6 +283,57 @@ def compute_dsa_indexer_loss(
     return indexer_loss
 
 
+def compute_attention_scores_with_cp(q, k, attn_bias, scale, heads_k_stride = 1):
+    """ 
+    compute attention scores of q_local @ k_global with CP all-gather backend
+    parallel on n_heads dimension 
+    """
+    pg = parallel_state.get_context_parallel_group()
+    cp_size = parallel_state.get_context_parallel_world_size()
+
+    sq_local, b, nheads, hn_q = q.shape
+    sk_local, _, nheads_k, hn_k = k.shape
+    sk_global = sk_local * cp_size
+    
+    assert nheads % nheads_k == 0 and nheads_k % heads_k_stride == 0
+
+    comm = AllGatherComm(group=pg)
+    attns = torch.zeros(b, heads_k_stride, sq_local, sk_global, dtype=q.dtype, device=q.device)
+
+    k_buffer = torch.empty(
+        (sk_global, b, heads_k_stride, hn_k),
+        dtype=k.dtype,
+        device=k.device
+    )
+    k_buffer_copy = torch.empty_like(k_buffer)
+    k_0 = k[:, :, :heads_k_stride].contiguous()
+    comm.all_gather(k_buffer_copy, k_0)
+
+    attn_bias = attn_bias.expand(-1, heads_k_stride * (nheads // nheads_k), -1, -1)
+
+    for i in range(0, nheads_k, heads_k_stride):
+        comm.wait()
+        k_buffer, k_buffer_copy = k_buffer_copy, k_buffer
+        if i < nheads_k - heads_k_stride:
+            kvsl = i + heads_k_stride
+            kvsr = kvsl + heads_k_stride
+            send_k = k[:, :, kvsl:kvsr].contiguous()
+            comm.all_gather(k_buffer_copy, send_k)
+        q_i = q[:, :, i * nheads // nheads_k : (i + heads_k_stride) * nheads // nheads_k]
+        k_i = k_buffer
+
+        _q_i = einops.rearrange(q_i, 's b h d -> b h s d')
+        _k_i = einops.rearrange(k_i, 's b h d -> b h d s')
+        attn_i = torch.matmul(_q_i.float(), _k_i.float()) * scale + attn_bias
+        attn_i = torch.nn.functional.softmax(attn_i, dim=-1, dtype=torch.float32)
+
+        attns = attns + attn_i
+    
+    attns = torch.sum(attns, dim=1)
+
+    return attns
+
+
 class DSAIndexerLossAutoScaler(torch.autograd.Function):
     """An AutoScaler that triggers the backward pass and scales the grad for indexer loss.
 
@@ -496,7 +578,15 @@ class DSAIndexer(MegatronModule):
         # Compute attention scores: q @ k^T
         # [seqlen_q, batch, index_n_heads, index_head_dim] @ [seqlen_k, batch, index_head_dim]^T
         #   -> [seqlen_q, batch, index_n_heads, seqlen_k]
-        index_scores = torch.einsum('sbhd,tbd->sbht', q.float(), k.float())
+        cp_size = parallel_state.get_context_parallel_world_size()
+        if cp_size == 1:
+            index_scores = torch.einsum('sbhd,tbd->sbht', q.float(), k.float())
+        else:
+            # because k is small (only 1 head), do just one all_gather
+            k_buffer = torch.cat(torch.distributed.nn.functional.all_gather(k, group=self.pg_collection.cp), dim=0)  # k_buffer: [[chunk_0, chunk_3, chunk_1, chunk_2], batch, index_head_dim]
+            index_scores = torch.einsum('sbhd,tbd->sbht', q.float(), k_buffer.float()) # [s_q_local, batch, index_n_heads, s_k_global]
+            # rank 0: q [chunk_0, chunk_3], k[chunk_0, chunk_3, chunk_1, chunk_2]
+            # rank 1: q [chunk_1, chunk_2], k[chunk_0, chunk_3, chunk_1, chunk_2]
 
         # Apply ReLU activation.
         index_scores = torch.relu(index_scores)
@@ -606,7 +696,10 @@ class DSAIndexer(MegatronModule):
         # =========================================
         # Select top-k indices
         # =========================================
-        topk_k = min(self.index_topk, seqlen)
+        cp_size = parallel_state.get_context_parallel_world_size()
+
+        seqlen_k_global = k.shape[0] * cp_size
+        topk_k = min(self.index_topk, seqlen_k_global)
         # [batch, seqlen, index_topk]
         topk_indices = index_scores.topk(topk_k, dim=-1)[1]
 
@@ -687,6 +780,49 @@ def unfused_dsa_fn(query, key, value, topk_indices, softmax_scale):
     output = output.reshape(sq, b, np * hnv)
     return output
 
+def get_causal_mask(sq, skv, device):
+    cp_size = parallel_state.get_context_parallel_world_size()
+    cp_rank = parallel_state.get_context_parallel_rank()
+    skv_global = skv * cp_size
+
+    if cp_size == 1:
+        causal_mask = torch.triu(
+            torch.ones((sq, skv), dtype=torch.bool, device=device),
+            diagonal=1,
+        )
+    else:
+        sq_half = sq // 2
+        global_q_positions = torch.cat([
+            torch.arange(cp_rank * sq_half, (cp_rank + 1) * sq_half, device=device),
+            torch.arange(skv_global - (cp_rank + 1) * sq_half, skv_global - cp_rank * sq_half, device=device)
+        ])
+        
+        global_k_positions = torch.arange(skv_global, device=device)
+        # [sq, 1] < [1, skv_global] -> [sq, skv_global]
+        causal_mask = global_q_positions.unsqueeze(1) < global_k_positions.unsqueeze(0)
+        # convert to zz mask
+        chunked = causal_mask.chunk(dim=1, chunks=cp_size * 2)
+        causal_mask = [_x for _p in zip(chunked[:cp_size], reversed(chunked[cp_size:])) for _x in _p]
+        causal_mask = torch.cat(causal_mask, dim=1)
+
+    return causal_mask
+
+def unfused_dsa_fn_with_cp(query, key, value, topk_indices, softmax_scale):
+    pg = parallel_state.get_context_parallel_group()
+
+    sq, b, np, hn = query.size()
+    skv = key.size(0)
+    hnv = value.size(3)
+    
+    topk = topk_indices.shape[-1]
+    topk_indices = topk_indices.expand(-1, key.shape[2], -1, -1).contiguous()
+    causal_masks = get_causal_mask(sq, skv, query.device)
+    casual_masks = causal_masks.expand(-1, key.shape[2], -1, -1).contiguous()
+    output = AttentionFuncionWithContextParallel.apply(
+        query, key, value, topk_indices, casual_masks, value.shape[3], topk, 0.0, softmax_scale, pg
+    )
+    return output.reshape(sq, b, np * hnv)
+
 
 class DSAttention(MegatronModule):
     """
@@ -768,18 +904,17 @@ class DSAttention(MegatronModule):
             # Generate upper triangular mask with -inf above diagonal, 0 elsewhere
             # torch.triu with diagonal=1 creates upper triangular matrix (excluding main diagonal)
             # float_mask [sq, skv]
-            float_mask = torch.triu(
-                torch.full((sq, skv), float('-inf'), dtype=torch.float32, device=x.device),
-                diagonal=1,
-            )
+            mask = get_causal_mask(sq, skv, x.device)
         else:
-            assert attention_mask.shape == (b, 1, sq, skv), 'attention_mask shape mismatch'
+            skv_global = skv * parallel_state.get_context_parallel_world_size()
+            assert attention_mask.shape == (b, 1, sq, skv_global), 'attention_mask shape mismatch'
             # [b, 1, sq, skv] -> [b, sq, skv]
             mask = attention_mask.squeeze()
-            # float_mask [b, sq, skv]
-            float_mask = torch.zeros_like(mask, dtype=torch.float32).masked_fill(
-                mask, float('-inf')
-            )
+
+        # float_mask [b, sq, skv]
+        float_mask = torch.zeros_like(mask, dtype=torch.float32).masked_fill(
+            mask, float('-inf')
+        )
 
         # ===================================
         # Get index scores and top-k indices
@@ -791,7 +926,7 @@ class DSAttention(MegatronModule):
         # ===================================
         # Run sparse attention kernel
         # ===================================
-        output = unfused_dsa_fn(query, key, value, topk_indices, self.softmax_scale)
+        output = unfused_dsa_fn_with_cp(query, key, value, topk_indices, self.softmax_scale)
 
         # ===================================
         # Attach indexer loss
diff --git a/megatron/core/transformer/multi_latent_attention.py b/megatron/core/transformer/multi_latent_attention.py
index 3953d933b..84301ed54 100644
--- a/megatron/core/transformer/multi_latent_attention.py
+++ b/megatron/core/transformer/multi_latent_attention.py
@@ -6,6 +6,7 @@ from dataclasses import dataclass
 from typing import NoReturn, Optional, Union
 
 import torch
+import torch.nn.functional as F
 
 try:
     from einops import rearrange
diff --git a/megatron/core/transformer/tilelang_kernel/__init__.py b/megatron/core/transformer/tilelang_kernel/__init__.py
new file mode 100644
index 000000000..d8f2425f0
--- /dev/null
+++ b/megatron/core/transformer/tilelang_kernel/__init__.py
@@ -0,0 +1,10 @@
+# Code is adopted from tilelang/examples/deepseek_v32
+# transformer/tilelang_kernel/__init__.py
+
+from .sparse_mla_fwd import sparse_mla_fwd_interface
+from .sparse_mla_bwd import sparse_mla_bwd
+
+__all__ = [
+    "sparse_mla_fwd_interface",
+    "sparse_mla_bwd",
+]
diff --git a/megatron/core/transformer/tilelang_kernel/sparse_mla_bwd.py b/megatron/core/transformer/tilelang_kernel/sparse_mla_bwd.py
new file mode 100644
index 000000000..a0972a749
--- /dev/null
+++ b/megatron/core/transformer/tilelang_kernel/sparse_mla_bwd.py
@@ -0,0 +1,274 @@
+# Code is adopted from tilelang/examples/deepseek_v32
+# ruff: noqa
+import tilelang
+from tilelang import language as T
+import torch
+
+@tilelang.jit(out_idx=[-1])
+def preprocess(
+    B,
+    S,
+    H,
+    D,
+    block_ND=32,
+    num_stages=5,
+    dtype=T.bfloat16,
+    accum_dtype=T.float32,
+):
+    assert dtype == T.bfloat16
+    assert accum_dtype == T.float32
+    shape = [B, S, H, D]
+
+    @T.prim_func
+    def preprocess_kernel(
+        O: T.Tensor(shape, dtype),
+        dO: T.Tensor(shape, dtype),
+        Delta: T.Tensor([B, S, H], accum_dtype),
+    ):
+        with T.Kernel(H, T.ceildiv(S, block_ND), B) as (bx, by, bz):
+            o = T.alloc_fragment([block_ND, block_ND], accum_dtype)
+            do = T.alloc_fragment([block_ND, block_ND], accum_dtype)
+            delta = T.alloc_fragment([block_ND], accum_dtype)
+            acc = T.alloc_fragment([block_ND, block_ND], accum_dtype)
+            T.clear(acc)
+            for k in T.Pipelined(T.ceildiv(D, block_ND), num_stages=num_stages):
+                T.copy(O[bz, by * block_ND : (by + 1) * block_ND, bx, k * block_ND : (k + 1) * block_ND], o)
+                T.copy(dO[bz, by * block_ND : (by + 1) * block_ND, bx, k * block_ND : (k + 1) * block_ND], do)
+                for i, j in T.Parallel(block_ND, block_ND):
+                    acc[i, j] += o[i, j] * do[i, j]
+            T.reduce_sum(acc, delta, 1)
+            T.copy(delta, Delta[bz, by * block_ND : (by + 1) * block_ND, bx])
+
+    return preprocess_kernel
+
+
+@tilelang.jit(out_idx=[-1])
+def postprocess(
+    B,
+    S_kv,
+    D,
+    D_tail,
+    kv_group=1,
+    block_N=64,
+    threads=256,
+    dtype=T.bfloat16,
+    accum_dtype=T.float32,
+):
+    assert dtype == T.bfloat16
+    assert accum_dtype == T.float32
+    dkv_shape = [B, S_kv, kv_group, D + D_tail]
+
+    @T.prim_func
+    def postprocess_kernel(
+        dKV: T.Tensor(dkv_shape, accum_dtype),
+        dKV_out: T.Tensor(dkv_shape, dtype),
+    ):
+        with T.Kernel(T.ceildiv(S_kv, block_N), kv_group, B, threads=threads) as (bx, by, bz):
+            T.copy(
+                dKV[bz, bx * block_N : (bx + 1) * block_N, by, :],
+                dKV_out[bz, bx * block_N : (bx + 1) * block_N, by, :],
+            )
+
+    return postprocess_kernel
+
+
+@tilelang.jit(
+    out_idx=[-2],
+    pass_configs={
+        tilelang.PassConfigKey.TL_DISABLE_TMA_LOWER: True,
+        tilelang.PassConfigKey.TL_DISABLE_WARP_SPECIALIZED: True,
+        tilelang.PassConfigKey.TL_ENABLE_AGGRESSIVE_SHARED_MEMORY_MERGE: True,
+    },
+)
+def bwd(
+    B,
+    S,
+    S_kv,
+    H,
+    D,
+    D_tail,
+    topk,
+    kv_group=1,
+    sm_scale=None,
+    is_causal=True,
+    block_size=32,
+    num_stages=0,
+    threads=128,
+    indices_dtype=T.int32,
+    dtype=T.bfloat16,
+    accum_dtype=T.float32,
+    masks_dtype=T.bool,
+):
+    assert is_causal == True, "non-casual is not supported now"
+    assert topk % block_size == 0, "otherwise will load some index=0 thus causing wrong kv to be loaded"
+    assert dtype == T.bfloat16
+    assert accum_dtype == T.float32
+    assert indices_dtype == T.int32
+
+    if sm_scale is None:
+        sm_scale = (D + D_tail) ** (-0.5)
+    sm_scale_mul_reciprocal_log2 = sm_scale * 1.44269504  # log2(e)
+
+    H_kv = H // kv_group
+    q_shape = [B, S, H, D + D_tail]
+    k_shape = [B, S_kv, kv_group, D + D_tail]
+    o_shape = [B, S, H, D]
+    indices_shape = [B, S, kv_group, topk]
+    delta_shape = [B, S, H]
+    lse_shape = [B, S, H]
+    masks_shape = [B, S, kv_group, S_kv]
+    assert indices_dtype == T.int32
+    assert dtype == T.bfloat16
+    assert accum_dtype == T.float32
+
+    H = H_kv
+    padded_H = max(tilelang.math.next_power_of_2(H_kv), 16)
+    block_H = min(64, padded_H)
+    assert padded_H % block_H == 0
+    NH = padded_H // block_H
+    BS = block_size
+    NS = tilelang.cdiv(topk, block_size)
+
+    split_store = 2
+
+    @T.prim_func
+    def sparse_mla_bwd_kernel(
+        Q: T.Tensor(q_shape, dtype),
+        KV: T.Tensor(k_shape, dtype),
+        dO: T.Tensor(o_shape, dtype),
+        Indices: T.Tensor(indices_shape, indices_dtype),
+        Masks: T.Tensor(masks_shape, masks_dtype),
+        Lse: T.Tensor(lse_shape, accum_dtype),
+        Delta: T.Tensor(delta_shape, accum_dtype),
+        dQ: T.Tensor(q_shape, dtype),
+        dKV: T.Tensor(k_shape, accum_dtype),
+    ):
+        with T.Kernel(S, B, kv_group * NH, threads=threads) as (s_i, by, bz):
+            Q_shared = T.alloc_shared([block_H, D], dtype)
+            Q_tail_shared = T.alloc_shared([block_H, D_tail], dtype)
+            KV_shared = T.alloc_shared([BS, D], dtype)
+            KV_tail_shared = T.alloc_shared([BS, D_tail * 4], dtype)
+            dO_shared = T.alloc_shared([block_H, D], dtype)
+            mask = T.alloc_fragment([BS], "bool")
+
+            P_shared_cast = T.alloc_shared([block_H, BS], dtype)
+            dP_shared_cast = T.alloc_shared([block_H, BS], dtype)
+            dQ_shared = T.alloc_shared([block_H, D], dtype)
+            dQ_tail_shared = T.alloc_shared([block_H, D_tail], dtype)
+
+            acc_p = T.alloc_fragment([block_H, BS], accum_dtype)
+            acc_dp = T.alloc_fragment([block_H, BS], accum_dtype)
+            acc_dq = T.alloc_fragment([block_H, D], accum_dtype)
+            acc_dq_tail = T.alloc_fragment([block_H, D_tail * 4], accum_dtype)
+            acc_dkv = T.alloc_fragment([BS, D], accum_dtype)
+            acc_dkv_tail = T.alloc_fragment([BS, D_tail], accum_dtype)
+            acc_dkv_shared = T.alloc_shared([BS // split_store, D], accum_dtype)
+            acc_dkv_tail_shared = T.alloc_shared([BS // split_store, D_tail], accum_dtype)
+
+            T.copy(Q[by, s_i, bz * block_H : (bz + 1) * block_H, :D], Q_shared)
+            T.copy(Q[by, s_i, bz * block_H : (bz + 1) * block_H, D:], Q_tail_shared)
+            T.copy(dO[by, s_i, bz * block_H : (bz + 1) * block_H, :D], dO_shared)
+
+            T.clear(acc_dq)
+            T.clear(acc_dq_tail)
+
+            # Process each block of indices
+            for i_i in T.Pipelined(NS, num_stages=num_stages):
+                # Compute attention scores
+                for bi_i in T.Parallel(BS):
+                    mask[bi_i] = Masks[by, s_i, bz // NH, Indices[by, s_i, bz // NH, i_i * BS + bi_i]]
+
+                for h_i, bi_i in T.Parallel(block_H, BS):
+                    acc_p[h_i, bi_i] = T.if_then_else(mask[bi_i], -T.infinity(acc_p.dtype), 0)
+
+                # Load KV, V for this block of indices
+                for bi_i, d_i in T.Parallel(BS, D):
+                    KV_shared[bi_i, d_i] = KV[by, Indices[by, s_i, bz // NH, i_i * BS + bi_i], bz // NH, d_i]
+
+                T.gemm(Q_shared, KV_shared, acc_p, transpose_B=True, policy=T.GemmWarpPolicy.FullCol)
+
+                for bi_i, d_i in T.Parallel(BS, D_tail):
+                    KV_tail_shared[bi_i, d_i] = KV[by, Indices[by, s_i, bz // NH, i_i * BS + bi_i], bz // NH, D + d_i]
+                T.gemm(Q_tail_shared, KV_tail_shared[:, :D_tail], acc_p, transpose_B=True, policy=T.GemmWarpPolicy.FullCol)
+
+                for h_i, bi_i in T.Parallel(block_H, BS):
+                    acc_p[h_i, bi_i] = T.exp2(acc_p[h_i, bi_i] * sm_scale_mul_reciprocal_log2 - Lse[by, s_i, bz * block_H + h_i])
+
+                T.copy(acc_p, P_shared_cast)
+
+                T.gemm(dO_shared, KV_shared, acc_dp, transpose_B=True, policy=T.GemmWarpPolicy.FullCol, clear_accum=True)
+
+                for h_i, bi_i in T.Parallel(block_H, BS):
+                    acc_dp[h_i, bi_i] = acc_p[h_i, bi_i] * (acc_dp[h_i, bi_i] - Delta[by, s_i, bz * block_H + h_i]) * sm_scale
+
+                T.copy(acc_dp, dP_shared_cast)
+                T.gemm(dP_shared_cast, KV_shared, acc_dq, policy=T.GemmWarpPolicy.FullCol)
+                T.gemm(dP_shared_cast, KV_tail_shared, acc_dq_tail, policy=T.GemmWarpPolicy.FullCol)
+
+                T.gemm(dP_shared_cast, Q_shared, acc_dkv, transpose_A=True, policy=T.GemmWarpPolicy.FullCol, clear_accum=True)
+                T.gemm(P_shared_cast, dO_shared, acc_dkv, transpose_A=True, policy=T.GemmWarpPolicy.FullCol)
+
+                T.clear(acc_dkv_tail)
+                T.gemm(dP_shared_cast, Q_tail_shared, acc_dkv_tail, transpose_A=True, policy=T.GemmWarpPolicy.FullCol)
+
+                for s in range(split_store):
+                    for bi_i, d_i in T.Parallel(BS, D):
+                        if bi_i < BS // split_store:
+                            acc_dkv_shared[bi_i, d_i] = acc_dkv[bi_i + s * (BS // split_store), d_i]
+
+                    for bi_i, d_i in T.Parallel(BS, D_tail):
+                        if bi_i < BS // split_store:
+                            acc_dkv_tail_shared[bi_i, d_i] = acc_dkv_tail[bi_i + s * (BS // split_store), d_i]
+
+                    for bi_i, d_i in T.Parallel(BS // split_store, D // 4):
+                        T.atomic_addx4(
+                            dKV[by, Indices[by, s_i, bz // NH, i_i * BS + bi_i + s * (BS // split_store)], bz // NH, d_i * 4],
+                            acc_dkv_shared[bi_i, d_i * 4],
+                        )
+
+                    # Atomically update dKV, dKV_tail tensors
+                    for bi_i, d_i in T.Parallel(BS // split_store, D_tail // 4):
+                        T.atomic_addx4(
+                            dKV[by, Indices[by, s_i, bz // NH, i_i * BS + bi_i + s * (BS // split_store)], bz // NH, D + d_i * 4],
+                            acc_dkv_tail_shared[bi_i, d_i * 4],
+                        )
+
+            # Store the accumulated dQ
+            T.copy(acc_dq, dQ_shared)
+            T.copy(acc_dq_tail[:, :D_tail], dQ_tail_shared)
+
+            T.copy(dQ_shared, dQ[by, s_i, bz * block_H : (bz + 1) * block_H, :D])
+            T.copy(dQ_tail_shared, dQ[by, s_i, bz * block_H : (bz + 1) * block_H, D:])
+
+    return sparse_mla_bwd_kernel
+
+
+def sparse_mla_bwd(q, kv, o, do, indices, masks, lse, dim_v, sm_scale=None, is_casual=True, return_kernel=False, delta=None):
+    assert q.is_contiguous()
+    assert kv.is_contiguous()
+    assert indices.is_contiguous()
+    assert lse.is_contiguous()
+    B, S, H, dim_plus_tail_dim = q.shape
+    _, S_kv, kv_group, _ = kv.shape
+    assert kv.shape[-1] == dim_plus_tail_dim
+    assert kv.shape[0] == B
+    # dim should be assigned
+    D = dim_v
+
+    D_tail = dim_plus_tail_dim - D
+    topk = indices.shape[-1]
+    assert indices.shape == (B, S, kv_group, topk)
+    assert lse.shape == (B, S, H)
+
+    # Get kernels
+    preprocess_kernel = preprocess(B, S, H, D)
+    bwd_kernel = bwd(B, S, S_kv, H, D, D_tail, topk, kv_group, sm_scale, is_casual)
+    postprocess_kernel = postprocess(B, S_kv, D, D_tail, kv_group)
+
+    if delta is None:
+        delta = preprocess_kernel(o, do)
+    dkv = torch.zeros_like(kv, dtype=torch.float32)
+    dq = bwd_kernel(q, kv, do, indices, masks, lse, delta, dkv)
+    dkv = postprocess_kernel(dkv)
+
+    return dq, dkv
diff --git a/megatron/core/transformer/tilelang_kernel/sparse_mla_fwd.py b/megatron/core/transformer/tilelang_kernel/sparse_mla_fwd.py
new file mode 100644
index 000000000..1f114117b
--- /dev/null
+++ b/megatron/core/transformer/tilelang_kernel/sparse_mla_fwd.py
@@ -0,0 +1,190 @@
+# Code is adopted from tilelang/examples/deepseek_v32
+# ruff: noqa
+import torch
+import tilelang
+from tilelang import language as T
+
+@tilelang.jit(
+    out_idx=[-2, -1],
+    pass_configs={
+        tilelang.PassConfigKey.TL_DISABLE_TMA_LOWER: True,
+        tilelang.PassConfigKey.TL_DISABLE_WARP_SPECIALIZED: True,
+    },
+)
+def sparse_mla_fwd(
+    heads,
+    dim,
+    tail_dim,
+    topk,
+    kv_group=1,
+    sm_scale=None,
+    is_causal=True,
+    CP0=True,
+    block_I=64,
+    num_stages=2,
+    threads=256,
+):
+    assert dim == tilelang.math.next_power_of_2(dim), f"haven't check padding correctness yet, dim={dim}"
+    assert tail_dim == tilelang.math.next_power_of_2(tail_dim), f"haven't check padding correctness yet, dim={tail_dim}"
+    assert is_causal == True, "non-casual is not supported"
+    assert topk % block_I == 0, "otherwise will load some index=0 thus causing wrong kv to be loaded"
+    if sm_scale is None:
+        sm_scale = (1.0 / (dim + tail_dim)) ** 0.5 * 1.44269504  # log2(e)
+    else:
+        sm_scale = sm_scale * 1.44269504  # log2(e)
+
+    batch = T.dynamic("batch")
+    seq_len = T.dynamic("seq_len")
+    seq_len_kv = T.dynamic("seq_len_kv")
+
+    head_kv = heads // kv_group
+    q_shape = [batch, seq_len, heads, dim + tail_dim]
+    kv_shape = [batch, seq_len_kv, kv_group, dim + tail_dim]
+    o_shape = [batch, seq_len, heads, dim]
+    indices_shape = [batch, seq_len, kv_group, topk]
+    lse_shape = [batch, seq_len, heads]
+    masks_shape = [batch, seq_len, kv_group, seq_len_kv]
+
+    masks_dtype = T.bool
+    indices_dtype = T.int32
+    dtype = T.bfloat16
+    accum_dtype = T.float32
+
+    G = kv_group
+    H = head_kv
+    padded_H = max(tilelang.math.next_power_of_2(head_kv), 16)
+    if padded_H != H:
+        assert kv_group == 1, (
+            "here we solve the H padding automatically, other wise you should handle Q copy and Output copy with your mask (when kv_group == 1, use g_i * padded_H:(g_i+1) * padded_H would be handled automatically)"
+        )
+    BI = block_I
+    NI = tilelang.cdiv(topk, block_I)
+    D = dim
+    D_tail = tail_dim
+
+    if head_kv > 64:
+        assert head_kv % 64 == 0, "head_kv should be a multiple of 64"
+        REPLICATE_H = head_kv // 64
+    else:
+        REPLICATE_H = 1
+
+    H_per_block = padded_H if REPLICATE_H == 1 else 64
+
+    @T.prim_func
+    def main(
+        Q: T.Tensor(q_shape, dtype),  # type: ignore
+        KV: T.Tensor(kv_shape, dtype),  # type: ignore
+        Indices: T.Tensor(indices_shape, indices_dtype),  # type: ignore
+        Masks: T.Tensor(masks_shape, masks_dtype), # type: ignore
+        Output: T.Tensor(o_shape, dtype),  # type: ignore
+        Lse: T.Tensor(lse_shape, accum_dtype),  # type: ignore
+    ):
+        with T.Kernel(seq_len * REPLICATE_H, batch, kv_group, threads=threads) as (
+            bx,
+            by,
+            bz,
+        ):
+            Q_shared = T.alloc_shared([H_per_block, D], dtype)
+            Q_tail_shared = T.alloc_shared([H_per_block, D_tail], dtype)
+            KV_shared = T.alloc_shared([BI, D], dtype)
+            K_tail_shared = T.alloc_shared([BI, D_tail], dtype)
+            O_shared = T.alloc_shared([H_per_block, D], dtype)
+            Lse_shared = T.alloc_shared([H_per_block], accum_dtype)
+            mask = T.alloc_fragment([BI], "bool")
+
+            acc_o = T.alloc_fragment([H_per_block, D], accum_dtype)
+            acc_s = T.alloc_fragment([H_per_block, BI], accum_dtype)
+            S_shared = T.alloc_shared([H_per_block, BI], dtype)
+            sumexp = T.alloc_fragment([H_per_block], accum_dtype)
+            sumexp_i = T.alloc_fragment([H_per_block], accum_dtype)
+            alpha = T.alloc_fragment([H_per_block], accum_dtype)
+            m_i = T.alloc_fragment([H_per_block], accum_dtype)
+            m_i_prev = T.alloc_fragment([H_per_block], accum_dtype)
+
+            T.fill(acc_o, 0)
+            T.fill(sumexp, 0)
+            T.fill(m_i, -(2**30))  # avoid -inf - inf to cause nan
+
+            b_i, g_i = by, bz
+            s_i = bx if REPLICATE_H == 1 else (bx // REPLICATE_H)
+            q_i = s_i
+
+            H0 = g_i * padded_H + (0 if REPLICATE_H == 1 else (bx % REPLICATE_H) * 64)
+            H1 = H0 + H_per_block
+
+            T.copy(Q[b_i, s_i, H0:H1, :D], Q_shared)
+            T.copy(Q[b_i, s_i, H0:H1, D:], Q_tail_shared)
+
+            for i_i in T.Pipelined(NI, num_stages=num_stages):
+                for bi_i in T.Parallel(BI):
+                    mask[bi_i] = Masks[b_i, s_i, g_i, Indices[b_i, s_i, g_i, i_i * BI + bi_i]]
+                for bi_i, d_i in T.Parallel(BI, D):
+                    KV_shared[bi_i, d_i] = KV[b_i, Indices[b_i, s_i, g_i, i_i * BI + bi_i], g_i, d_i]
+                for bi_i, d_i in T.Parallel(BI, D_tail):
+                    K_tail_shared[bi_i, d_i] = KV[b_i, Indices[b_i, s_i, g_i, i_i * BI + bi_i], g_i, D + d_i]
+                for h_i, bi_i in T.Parallel(H_per_block, BI):
+                    acc_s[h_i, bi_i] = T.if_then_else(mask[bi_i], -T.infinity(acc_s.dtype), 0)
+                T.gemm(
+                    Q_shared,
+                    KV_shared,
+                    acc_s,
+                    transpose_B=True,
+                    policy=T.GemmWarpPolicy.FullRow,
+                )
+                T.gemm(
+                    Q_tail_shared,
+                    K_tail_shared,
+                    acc_s,
+                    transpose_B=True,
+                    policy=T.GemmWarpPolicy.FullRow,
+                )
+                T.copy(m_i, m_i_prev)
+                T.reduce_max(acc_s, m_i, dim=1, clear=False)
+                for h_i in T.Parallel(H_per_block):
+                    m_i[h_i] = T.max(m_i[h_i], m_i_prev[h_i])
+                for h_i in T.Parallel(H_per_block):
+                    alpha[h_i] = T.exp2((m_i_prev[h_i] - m_i[h_i]) * sm_scale)
+                for h_i, bi_i in T.Parallel(H_per_block, BI):
+                    acc_s[h_i, bi_i] = T.exp2(acc_s[h_i, bi_i] * sm_scale - m_i[h_i] * sm_scale)
+                T.reduce_sum(acc_s, sumexp_i, dim=1)  # is this a accumulate operator?
+                for h_i in T.Parallel(H_per_block):
+                    sumexp[h_i] = sumexp[h_i] * alpha[h_i] + sumexp_i[h_i]
+                for h_i, d_i in T.Parallel(H_per_block, D):
+                    acc_o[h_i, d_i] = acc_o[h_i, d_i] * alpha[h_i]
+
+                T.copy(acc_s, S_shared)
+                T.gemm(S_shared, KV_shared, acc_o, policy=T.GemmWarpPolicy.FullRow)
+
+            # Rescale
+            for h_i, d_i in T.Parallel(H_per_block, D):
+                acc_o[h_i, d_i] /= sumexp[h_i]
+            for h_i in T.Parallel(H_per_block):
+                sumexp[h_i] = T.log2(sumexp[h_i]) + m_i[h_i] * sm_scale
+
+            T.copy(acc_o, O_shared)
+            T.copy(acc_o, Output[b_i, s_i, H0:H1, :])
+            T.copy(sumexp, Lse_shared)
+            T.copy(sumexp, Lse[b_i, s_i, H0:H1])
+
+    return main
+
+
+def sparse_mla_fwd_interface(q, kv, indices, masks, d_v, sm_scale=None, return_p_sum: bool = False, block_I=64, num_stages=2, threads=256):
+    is_casual = True
+    assert return_p_sum == False, "This kernel file is for fwd only"
+    assert q.is_contiguous() and kv.is_contiguous() and indices.is_contiguous()
+    batch, seq_len, heads, dim_plus_tail_dim = q.shape
+    _, seq_len_kv, kv_group, _ = kv.shape
+
+    assert kv.shape[-1] == dim_plus_tail_dim
+    tail_dim = dim_plus_tail_dim - d_v
+    assert kv.shape[0] == batch
+    _, _, _, topk = indices.shape
+    assert indices.shape == (batch, seq_len, kv_group, topk)
+    assert masks.shape == (batch, seq_len, kv_group, seq_len_kv)
+
+    kernel = sparse_mla_fwd(
+        heads, d_v, tail_dim, topk, kv_group, sm_scale, is_casual, block_I=block_I, num_stages=num_stages, threads=threads
+    )
+    out, lse = kernel(q, kv, indices, masks)
+    return out, lse
diff --git a/megatron/core/transformer/transformer_config.py b/megatron/core/transformer/transformer_config.py
index a3a167549..98391fda6 100644
--- a/megatron/core/transformer/transformer_config.py
+++ b/megatron/core/transformer/transformer_config.py
@@ -918,9 +918,9 @@ class TransformerConfig(ModelParallelConfig):
                     f" but got {self.context_parallel_size=}."
                 )
         elif self.experimental_attention_variant == "dsa":
-            assert (
-                self.context_parallel_size == 1
-            ), "Currently context parallelism is not supported by DSAttention!"
+            # assert (
+            #     self.context_parallel_size == 1
+            # ), "Currently context parallelism is not supported by DSAttention!"
             assert not self.apply_rope_fusion, "RoPE fusion is not supported for DSAttention"
 
         if self.fp8:
